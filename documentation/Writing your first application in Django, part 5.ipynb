{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59080f3e",
   "metadata": {},
   "source": [
    "## Presentando las pruebas automatizadas\n",
    "### ¿Qué son las pruebas automatizadas?\n",
    "\n",
    "Las pruebas son rutinas que verifican el funcionamiento de su código.\n",
    "\n",
    "Las pruebas se ejecutan en diferentes niveles. Algunas pruebas se pueden aplicar a un pequeño detalle (¿Un método de modelo particular retorna valores como se espera?) mientras que otras examinan el total funcionamiento del software (¿Una secuencia de entradas de usuarios en el sitio produce el resultado deseado?). No se diferencia del tipo de pruebas que ejecutó anteriormente en el Tutorial 2 utilizando el comando shell para examinar el comportamiento de un método, o ejecutar la aplicación e ingresar datos para comprobar cómo se comporta.\n",
    "\n",
    "Lo que es diferente en las pruebas automatizadas es que el trabajo de pruebas por usted lo hace el sistema. Usted crea un conjunto de pruebas una vez, después, a medida que modifica su aplicación, puede comprobar que el código todavía funciona como originalmente lo concibió, sin necesidad de pruebas manuales engorrosas.\n",
    "### El por qué es necesario crear pruebas\n",
    "\n",
    "Entonces, ¿por qué crear pruebas y por qué ahora?\n",
    "\n",
    "Usted podría pensar que ya con aprender Python y Django tiene bastante con qué ocuparse, y que tener que aprender y hacer aún más cosas podría ser abrumador y hasta innecesario. Después de todo, nuestra aplicación encuestas está funcionando ahora bastante bien, tomarse la molestia de crear pruebas automatizadas no va a hacer que funcione mejor. Si crear la aplicación encuestas es el último fragmento de programa de Django que hará, entonces es verdad, usted no necesita saber cómo crear pruebas automatizadas. Sin embargo, si ese no es el caso, ahora es un momento excelente para aprender.\n",
    "### Las pruebas le ahorrarán tiempo\n",
    "\n",
    "Hasta cierto punto, “comprobar de que parece funcionar” será una prueba satisfactoria. En una aplicación más sofisticada, es posible que tenga decenas de interacciones complejas entre los componentes.\n",
    "\n",
    "A change in any of those components could have unexpected consequences on the application’s behavior. Checking that it still “seems to work” could mean running through your code’s functionality with twenty different variations of your test data to make sure you haven’t broken something - not a good use of your time.\n",
    "\n",
    "Eso es especialmente cierto cuando las pruebas automatizadas pueden hacer esto por usted en cuestión de segundos. Si algo salió mal, las pruebas también ayudarán a identificar el código que está causando el comportamiento inesperado.\n",
    "\n",
    "A veces puede parecer un fastidio apartarse de su trabajo creativo y productivo de programación para afrontar la obligación poca atractiva y aburrida de escribir pruebas, particularmente cuando sabe que su código funciona de forma adecuada.\n",
    "\n",
    "Sin embargo, la tarea de escribir pruebas es mucho más gratificante que pasar horas probando su aplicación de forma manual o intentando identificar la causa de un problema recién introducido.\n",
    "### Las pruebas no solo identifican los problemas, los previenen\n",
    "\n",
    "Es un error pensar en las pruebas sólo como un aspecto negativo del desarrollo.\n",
    "\n",
    "Sin las pruebas, el propósito o el comportamiento previsto de una aplicación podría ser bastante incomprensible. Aún cuando es su propio código, usted a veces se encontrará buscando en él tratando de averiguar qué es exactamente lo que hace.\n",
    "\n",
    "Las pruebas cambian eso; iluminan su código desde adentro, y cuando algo sale mal, ellas se centran en la parte que salió mal; incluso si usted no se ha dado cuenta de que salió mal.\n",
    "### Las pruebas hacen más atractivo su código\n",
    "\n",
    "Es posible que haya creado un software brillante, pero descubrirá que muchos otros desarrolladores se negarán a examinarlo porque carece de pruebas; sin pruebas, no confiarán en él. Jacob Kaplan-Moss, uno de los desarrolladores originales de Django, dice: \"El código sin pruebas se rompe por diseño\".\n",
    "\n",
    "Que otros desarrolladores quieran ver pruebas en su software antes de que se lo tomen en serio es una razón más para que usted comience a escribir las pruebas.\n",
    "### Las pruebas ayudan a los equipos a trabajar en conjunto\n",
    "\n",
    "Los puntos anteriores están escritos desde el punto de vista de un único desarrollador que mantiene una aplicación. Las aplicaciones complejas serán mantenidas por equipos. Las pruebas garantizan que los colegas no rompan inadvertidamente su código (y que usted no rompa el de ellos sin saber). Si quiere ganarse la vida como un programador de Django, ¡Usted debe ser bueno escribiendo pruebas!\n",
    "### Estrategias básicas de pruebas\n",
    "\n",
    "Hay muchas maneras de abordar la escritura de pruebas.\n",
    "\n",
    "Algunos programadores siguen una disciplina llamada «desarrollo basado en pruebas»; en realidad escriben sus pruebas antes de escribir su código. Esto puede parecer contrario a la intuición, pero en realidad es similar a lo que la mayoría de la gente suele hacer: describen un problema y luego crean algún código para resolverlo. El desarrollo basado en pruebas formaliza el problema en un caso de prueba de Python.\n",
    "\n",
    "La mayoría de las veces, un principiante en pruebas va a crear código y luego decidirá que este debe tener algunas pruebas. Tal vez hubiera sido mejor escribir algunas pruebas antes, pero nunca es demasiado tarde para empezar.\n",
    "\n",
    "A veces es difícil saber por dónde empezar con la escritura de pruebas. Si usted ha escrito varias miles de líneas de Python, elegir algo para probar podría no ser fácil. En tal caso, es provechoso escribir su primera prueba la próxima vez que realice una modificación, ya sea cuando agregue una nueva funcionalidad o solucione un error.\n",
    "\n",
    "Así que vamos a hacer eso de inmediato.\n",
    "## Escribiendo nuestra primera prueba\n",
    "### Identificamos un error\n",
    "\n",
    "Afortunadamente, hay un pequeño error en la aplicación polls para que lo solucionemos de inmediato: el método Question.was_published_recently() retorna True si la Question fue publicada en el último día (que es correcto), pero también si el campo de pub_date de Question está en el futuro (que de hecho no lo está).\n",
    "\n",
    "Confirme el error usando el shell para verificar el método en una pregunta cuya fecha sea futura:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4fc8ebb",
   "metadata": {},
   "source": [
    "$ python manage.py shell"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97353b2f",
   "metadata": {},
   "source": [
    ">>> import datetime\n",
    ">>> from django.utils import timezone\n",
    ">>> from polls.models import Question\n",
    ">>> # create a Question instance with pub_date 30 days in the future\n",
    ">>> future_question = Question(pub_date=timezone.now() + datetime.timedelta(days=30))\n",
    ">>> # was it published recently?\n",
    ">>> future_question.was_published_recently()\n",
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6159a96",
   "metadata": {},
   "source": [
    "Puesto que las cosas en el futuro no son “recientes”, esto es claramente erróneo.\n",
    "## Cree una prueba para exponer el error\n",
    "\n",
    "Lo que acabamos de hacer en el comando shell para detectar el problema es exactamente lo que podemos hacer en una prueba automatizada, así que vamos a convertir eso en una prueba automatizada.\n",
    "\n",
    "Un lugar convencional para las pruebas de una aplicación se encuentra en el archivo tests.py de la aplicación; el sistema de pruebas encontrará automáticamente las pruebas en cualquier archivo cuyo nombre comience con test.\n",
    "\n",
    "Ponga lo siguiente en el archivo tests.py de la aplicación polls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0db497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/tests.py\n",
    "\n",
    "import datetime\n",
    "\n",
    "from django.test import TestCase\n",
    "from django.utils import timezone\n",
    "\n",
    "from .models import Question\n",
    "\n",
    "\n",
    "class QuestionModelTests(TestCase):\n",
    "    def test_was_published_recently_with_future_question(self):\n",
    "        \"\"\"\n",
    "        was_published_recently() returns False for questions whose pub_date\n",
    "        is in the future.\n",
    "        \"\"\"\n",
    "        time = timezone.now() + datetime.timedelta(days=30)\n",
    "        future_question = Question(pub_date=time)\n",
    "        self.assertIs(future_question.was_published_recently(), False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856add1",
   "metadata": {},
   "source": [
    "Aquí hemos creado una subclase django.test.TestCase con un método que crea una instancia de Pregunta con una fecha de publicación en el futuro. Luego verificamos el resultado de was_published_recently(), que debería ser Falso.\n",
    "## Ejecutando las pruebas\n",
    "\n",
    "En la terminal, podemos ejecutar nuestra prueba:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44709c8a",
   "metadata": {},
   "source": [
    "$ python manage.py test polls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf61d6e",
   "metadata": {},
   "source": [
    "y verás algo como:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58b42748",
   "metadata": {},
   "source": [
    "Creating test database for alias 'default'...\n",
    "System check identified no issues (0 silenced).\n",
    "F\n",
    "======================================================================\n",
    "FAIL: test_was_published_recently_with_future_question (polls.tests.QuestionModelTests)\n",
    "----------------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"/path/to/mysite/polls/tests.py\", line 16, in test_was_published_recently_with_future_question\n",
    "    self.assertIs(future_question.was_published_recently(), False)\n",
    "AssertionError: True is not False\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.001s\n",
    "\n",
    "FAILED (failures=1)\n",
    "Destroying test database for alias 'default'..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f45794",
   "metadata": {},
   "source": [
    "¿Error diferente?\n",
    "\n",
    "Si, en cambio, recibe un NameError aquí, es posible que se haya perdido un paso en la Parte 2 donde agregamos importaciones de fecha, hora y zona horaria a polls/models.py. Copie las importaciones de esa sección e intente ejecutar las pruebas nuevamente.\n",
    "\n",
    "Esto es lo que ocurrió:\n",
    "\n",
    "    manage.py test polls looked for tests in the polls application\n",
    "    encontró una subclase de la: clase: clase django.test.TestCase\n",
    "    creó una base de datos especial para las pruebas\n",
    "    buscó métodos de prueba; aquellos cuyos nombres comienzan con test\n",
    "    en test_was_published_recently_with_future_question creó una instancia Question cuyo campo de pub_date está 30 días en el futuro\n",
    "    … y utilizando el método assertIs() descubrió que was_published_recently() retorna True, aunque queríamos que retornara False\n",
    "\n",
    "La prueba nos informa que prueba falló e incluso la línea en la que se produjo el error.\n",
    "Solucionando el error¶\n",
    "\n",
    "Ya sabemos cuál es el problema: Question.was_published_recently() debe devolver False si su pub_date está en el futuro. Modifique el método models.py, de modo que este sólo retornará True si la fecha está también en el pasado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/models.py\n",
    "\n",
    "def was_published_recently(self):\n",
    "    now = timezone.now()\n",
    "    return now - datetime.timedelta(days=1) <= self.pub_date <= now\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15f53f68",
   "metadata": {},
   "source": [
    "Creating test database for alias 'default'...\n",
    "System check identified no issues (0 silenced).\n",
    ".\n",
    "----------------------------------------------------------------------\n",
    "Ran 1 test in 0.001s\n",
    "\n",
    "OK\n",
    "Destroying test database for alias 'default'..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72917e",
   "metadata": {},
   "source": [
    "Después de identificar un error, escribimos una prueba que lo expone y corregimos el error en el código para que nuestra prueba pase.\n",
    "\n",
    "Muchas otras cosas podrían salir mal con nuestra aplicación en el futuro, pero podemos estar seguros de que no reintroduciremos este error sin darnos cuenta, porque ejecutar la prueba nos avisará inmediatamente. Podemos considerar esta pequeña parte de la aplicación fijada de forma segura para siempre.\n",
    "### Pruebas más exhaustivas\n",
    "\n",
    "Ya que estamos aquí, podemos además precisar el método was_published_recently(); de hecho, sería verdaderamente vergonzoso si al corregir un error hubiéramos introducido otro.\n",
    "\n",
    "Agregue dos métodos de pruebas más a la misma clase, para probar el comportamiento del método de forma más completa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaefa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/tests.py\n",
    "\n",
    "def test_was_published_recently_with_old_question(self):\n",
    "    \"\"\"\n",
    "    was_published_recently() returns False for questions whose pub_date\n",
    "    is older than 1 day.\n",
    "    \"\"\"\n",
    "    time = timezone.now() - datetime.timedelta(days=1, seconds=1)\n",
    "    old_question = Question(pub_date=time)\n",
    "    self.assertIs(old_question.was_published_recently(), False)\n",
    "\n",
    "\n",
    "def test_was_published_recently_with_recent_question(self):\n",
    "    \"\"\"\n",
    "    was_published_recently() returns True for questions whose pub_date\n",
    "    is within the last day.\n",
    "    \"\"\"\n",
    "    time = timezone.now() - datetime.timedelta(hours=23, minutes=59, seconds=59)\n",
    "    recent_question = Question(pub_date=time)\n",
    "    self.assertIs(recent_question.was_published_recently(), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a576b31",
   "metadata": {},
   "source": [
    "Y ahora tenemos tres pruebas que confirman que Question.was_published_recently() retorna valores razonables para preguntas pasadas, recientes y futuras.\n",
    "\n",
    "Nuevamente, encuestas es una aplicación mínima, pero por muy compleja que crezca en el futuro y sea cual sea el otro código con el que interactúe, ahora tenemos alguna garantía de que el método para el que hemos escrito pruebas se comportará de la manera esperada.\n",
    "### Pruebe una vista\n",
    "\n",
    "La aplicación encuestas no discrimina en lo absoluto: publicará cualquier pregunta, incluyendo aquellas cuyo campo de pub_date esté en el futuro. Deberíamos actualizar esto. Establecer un pub_date en el futuro debería significar que la pregunta se publica en ese momento, pero que será invisible hasta entonces.\n",
    "### Una prueba para una vista\n",
    "\n",
    "Cuando solucionamos el error anterior, primero escribimos la prueba y luego el código para solucionarlo. De hecho, ese fue un ejemplo de desarrollo basado en pruebas, pero realmente no importa en qué orden hagamos el trabajo.\n",
    "\n",
    "En nuestra primera prueba, nos centramos detalladamente en el comportamiento interno del código. Para esta prueba, queremos comprobar su comportamiento como lo experimentaría un usuario a través de un navegador web.\n",
    "\n",
    "Antes de tratar de corregir algo, echemos un vistazo a las herramientas a nuestra disposición.\n",
    "### El cliente de prueba de Django\n",
    "\n",
    "Django proporciona una prueba Client para simular un usuario interactúando con el código al nivel de la vista. Podemos utilizarlo en tests.py o incluso en el shell.\n",
    "\n",
    "Empezaremos de nuevo con el shell, donde necesitamos hacer un par de cosas que no serán necesarias en tests.py. El primero es configurar el entorno de prueba en el shell:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "937178d8",
   "metadata": {},
   "source": [
    "$ python manage.py shell"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f296f2b",
   "metadata": {},
   "source": [
    ">>> from django.test.utils import setup_test_environment\n",
    ">>> setup_test_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef1fba",
   "metadata": {},
   "source": [
    "setup_test_environment() instala un procesador de plantillas que nos permitirá examinar algunos atributos adicionales en las respuestas, como respuesta.context, que de otro modo no estarían disponibles. Tenga en cuenta que este método no configura una base de datos de prueba, por lo que lo siguiente se ejecutará en la base de datos existente y el resultado puede diferir ligeramente según las preguntas que ya creó. Es posible que obtengas resultados inesperados si tu TIME_ZONE en settings.py no es correcto. Si no recuerda haberlo configurado antes, verifíquelo antes de continuar.\n",
    "\n",
    "A continuación necesitamos importar la clase de cliente de prueba (más adelante en tests.py usaremos la clase django.test.TestCase, que viene con su propio cliente, por lo que esto no será necesario):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6706dbb9",
   "metadata": {},
   "source": [
    ">>> from django.test import Client\n",
    ">>> # create an instance of the client for our use\n",
    ">>> client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec092f",
   "metadata": {},
   "source": [
    "Con eso listo, podemos pedirle al cliente que haga algún trabajo por nosotros:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33cfb0e7",
   "metadata": {},
   "source": [
    ">>> # get a response from '/'\n",
    ">>> response = client.get(\"/\")\n",
    "Not Found: /\n",
    ">>> # we should expect a 404 from that address; if you instead see an\n",
    ">>> # \"Invalid HTTP_HOST header\" error and a 400 response, you probably\n",
    ">>> # omitted the setup_test_environment() call described earlier.\n",
    ">>> response.status_code\n",
    "404\n",
    ">>> # on the other hand we should expect to find something at '/polls/'\n",
    ">>> # we'll use 'reverse()' rather than a hardcoded URL\n",
    ">>> from django.urls import reverse\n",
    ">>> response = client.get(reverse(\"polls:index\"))\n",
    ">>> response.status_code\n",
    "200\n",
    ">>> response.content\n",
    "b'\\n    <ul>\\n    \\n        <li><a href=\"/polls/1/\">What&#x27;s up?</a></li>\\n    \\n    </ul>\\n\\n'\n",
    ">>> response.context[\"latest_question_list\"]\n",
    "<QuerySet [<Question: What's up?>]>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070653a",
   "metadata": {},
   "source": [
    "## Mejorando nuestra vista\n",
    "\n",
    "La lista de encuestas muestra las encuestas que aún no están publicadas (es decir, aquellas que tienen una pub_date en el futuro). Vamos a solucionar eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/views.py\n",
    "\n",
    "class IndexView(generic.ListView):\n",
    "    template_name = \"polls/index.html\"\n",
    "    context_object_name = \"latest_question_list\"\n",
    "\n",
    "    def get_queryset(self):\n",
    "        \"\"\"Return the last five published questions.\"\"\"\n",
    "        return Question.objects.order_by(\"-pub_date\")[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e25ed",
   "metadata": {},
   "source": [
    "Tenemos que modificar el método get_queryset() y cambiarlo para que también compruebe la fecha comparándolo con timezone.now(). En primer lugar tenemos que añadir un import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/views.py\n",
    "\n",
    "from django.utils import timezone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f8125",
   "metadata": {},
   "source": [
    "y luego hay que modificar el método get_queryset como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a344273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/views.py\n",
    "\n",
    "def get_queryset(self):\n",
    "    \"\"\"\n",
    "    Return the last five published questions (not including those set to be\n",
    "    published in the future).\n",
    "    \"\"\"\n",
    "    return Question.objects.filter(pub_date__lte=timezone.now()).order_by(\"-pub_date\")[\n",
    "        :5\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca1a69",
   "metadata": {},
   "source": [
    "Question.objects.filter(pub_date__lte=timezone.now()) retorna un queryset que contiene Questions cuya pub_date es menor o igual a, es decir, anterior o igual a timezone.now.\n",
    "### Probando nuestra nueva vista\n",
    "\n",
    "Now you can satisfy yourself that this behaves as expected by firing up runserver, loading the site in your browser, creating Questions with dates in the past and future, and checking that only those that have been published are listed. You don’t want to have to do that every single time you make any change that might affect this - so let’s also create a test, based on our shell session above.\n",
    "\n",
    "Agregue lo siguiente a polls/tests.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33464167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/tests.py\n",
    "\n",
    "from django.urls import reverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ed344",
   "metadata": {},
   "source": [
    "y vamos a crear una función de atajo para crear preguntas, así como una nueva clase de pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/tests.py\n",
    "\n",
    "def create_question(question_text, days):\n",
    "    \"\"\"\n",
    "    Create a question with the given `question_text` and published the\n",
    "    given number of `days` offset to now (negative for questions published\n",
    "    in the past, positive for questions that have yet to be published).\n",
    "    \"\"\"\n",
    "    time = timezone.now() + datetime.timedelta(days=days)\n",
    "    return Question.objects.create(question_text=question_text, pub_date=time)\n",
    "\n",
    "\n",
    "class QuestionIndexViewTests(TestCase):\n",
    "    def test_no_questions(self):\n",
    "        \"\"\"\n",
    "        If no questions exist, an appropriate message is displayed.\n",
    "        \"\"\"\n",
    "        response = self.client.get(reverse(\"polls:index\"))\n",
    "        self.assertEqual(response.status_code, 200)\n",
    "        self.assertContains(response, \"No polls are available.\")\n",
    "        self.assertQuerySetEqual(response.context[\"latest_question_list\"], [])\n",
    "\n",
    "    def test_past_question(self):\n",
    "        \"\"\"\n",
    "        Questions with a pub_date in the past are displayed on the\n",
    "        index page.\n",
    "        \"\"\"\n",
    "        question = create_question(question_text=\"Past question.\", days=-30)\n",
    "        response = self.client.get(reverse(\"polls:index\"))\n",
    "        self.assertQuerySetEqual(\n",
    "            response.context[\"latest_question_list\"],\n",
    "            [question],\n",
    "        )\n",
    "\n",
    "    def test_future_question(self):\n",
    "        \"\"\"\n",
    "        Questions with a pub_date in the future aren't displayed on\n",
    "        the index page.\n",
    "        \"\"\"\n",
    "        create_question(question_text=\"Future question.\", days=30)\n",
    "        response = self.client.get(reverse(\"polls:index\"))\n",
    "        self.assertContains(response, \"No polls are available.\")\n",
    "        self.assertQuerySetEqual(response.context[\"latest_question_list\"], [])\n",
    "\n",
    "    def test_future_question_and_past_question(self):\n",
    "        \"\"\"\n",
    "        Even if both past and future questions exist, only past questions\n",
    "        are displayed.\n",
    "        \"\"\"\n",
    "        question = create_question(question_text=\"Past question.\", days=-30)\n",
    "        create_question(question_text=\"Future question.\", days=30)\n",
    "        response = self.client.get(reverse(\"polls:index\"))\n",
    "        self.assertQuerySetEqual(\n",
    "            response.context[\"latest_question_list\"],\n",
    "            [question],\n",
    "        )\n",
    "\n",
    "    def test_two_past_questions(self):\n",
    "        \"\"\"\n",
    "        The questions index page may display multiple questions.\n",
    "        \"\"\"\n",
    "        question1 = create_question(question_text=\"Past question 1.\", days=-30)\n",
    "        question2 = create_question(question_text=\"Past question 2.\", days=-5)\n",
    "        response = self.client.get(reverse(\"polls:index\"))\n",
    "        self.assertQuerySetEqual(\n",
    "            response.context[\"latest_question_list\"],\n",
    "            [question2, question1],\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb5060",
   "metadata": {},
   "source": [
    "Veamos algunas de estas más de cerca.\n",
    "\n",
    "En primer lugar es una función de atajo a las preguntas, create_question, para eliminar parte de la repetición del proceso de creación de preguntas.\n",
    "\n",
    "test_no_questions no crea ninguna pregunta, pero comprueba el mensaje: «No hay encuestas disponibles». y verifica que la lista_última_pregunta esté vacía. Tenga en cuenta que la clase django.test.TestCase proporciona algunos métodos de aserción adicionales. En estos ejemplos, utilizamos afirmarContains() y afirmarQuerySetEqual().\n",
    "\n",
    "En test_past_question creamos una pregunta y verificamos que aparezca en la lista.\n",
    "\n",
    "En test_future_question creamos una pregunta con una pub_date en el futuro. La base de datos se reinicia para cada método de prueba, por lo que la primera pregunta ya no está allí, de modo que de nuevo el índice no debería tener ninguna pregunta.\n",
    "\n",
    "Y así sucesivamente. En efecto, estamos utilizando las pruebas para contar una historia del ingreso de datos en el sitio administrativo y experiencia de usuario en el sitio, y comprobar que los resultados esperados se publican en cada estado y para cada nuevo cambio en el estado del sistema.\n",
    "### Probando la DetailView\n",
    "\n",
    "Lo que tenemos funciona bien; sin embargo, a pesar de que las preguntas futuras no aparecen en el índice, los usuarios pueden todavía llegar a ellas si saben o adivinan la dirección URL correcta. Así que tenemos que agregar una restricción similar a la DetailView:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/views.py\n",
    "\n",
    "class DetailView(generic.DetailView):\n",
    "    ...\n",
    "\n",
    "    def get_queryset(self):\n",
    "        \"\"\"\n",
    "        Excludes any questions that aren't published yet.\n",
    "        \"\"\"\n",
    "        return Question.objects.filter(pub_date__lte=timezone.now())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adcb00",
   "metadata": {},
   "source": [
    "Luego deberíamos agregar algunas pruebas para comprobar que se puede mostrar una pregunta cuya fecha de publicación está en el pasado y aquella con una fecha de publicación en el futuro no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# polls/tests.py\n",
    "\n",
    "class QuestionDetailViewTests(TestCase):\n",
    "    def test_future_question(self):\n",
    "        \"\"\"\n",
    "        The detail view of a question with a pub_date in the future\n",
    "        returns a 404 not found.\n",
    "        \"\"\"\n",
    "        future_question = create_question(question_text=\"Future question.\", days=5)\n",
    "        url = reverse(\"polls:detail\", args=(future_question.id,))\n",
    "        response = self.client.get(url)\n",
    "        self.assertEqual(response.status_code, 404)\n",
    "\n",
    "    def test_past_question(self):\n",
    "        \"\"\"\n",
    "        The detail view of a question with a pub_date in the past\n",
    "        displays the question's text.\n",
    "        \"\"\"\n",
    "        past_question = create_question(question_text=\"Past Question.\", days=-5)\n",
    "        url = reverse(\"polls:detail\", args=(past_question.id,))\n",
    "        response = self.client.get(url)\n",
    "        self.assertContains(response, past_question.question_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcc261",
   "metadata": {},
   "source": [
    "### Ideas para más pruebas\n",
    "\n",
    "Debemos agregar un método get_queryset similar a ResultsView y crear una nueva clase de pruebas para esa vista. Va a ser muy similar a lo que acabamos de crear; de hecho, habrá mucha repetición.\n",
    "\n",
    "También podríamos mejorar nuestra aplicación de otras maneras, agregando pruebas a lo largo del camino. Por ejemplo, es una tontería que se puedan publicar Questions en el sitio que no tienen Choices. Por lo tanto, nuestras vistas podrían comprobar esto y excluir dichas Questions. Nuestras pruebas crearían una Question sin Choices y luego probarían que no está publicada, así como crearían una Question similar con Choices y probarían que está publicada.\n",
    "\n",
    "Quizás se les debería permitir a los usuarios registrados en el sitio administrativo ver las Questions que no han sido publicadas, pero no a los visitantes ordinarios. Una vez más: lo que se tenga que agregar al software para lograr esto debería ir acompañado de una prueba, ya sea que escriba la prueba primero y luego haga que el código pase la prueba, o primero solucione la lógica en el código y luego escriba una prueba para probarlo.\n",
    "\n",
    "En un momento dado usted está obligado a examinar sus pruebas y a preguntarse si su código sufre de exceso de pruebas, lo que nos lleva a:\n",
    "### Cuando se trata de pruebas, más es mejor\n",
    "\n",
    "Podría parecer que nuestras pruebas están creciendo fuera de control. A este paso pronto habrá más código en nuestras pruebas que en nuestra aplicación y la repetición es poco estética en comparación con la refinada concisión del resto de nuestro código.\n",
    "\n",
    "No importa, deje que crezcan. Por lo general, usted puede escribir una prueba una vez y luego olvidarse de ella. Esta seguirá cumpliendo su función útil mientras usted continúe desarrollando su programa.\n",
    "\n",
    "A veces las pruebas tendrán que ser actualizadas. Supongamos que modificamos nuestras vistas de modo que sólo las Questions con Choices se publiquen. En ese caso, muchas de nuestras pruebas existentes fallarán, indicándonos exactamente qué pruebas necesitan ser modificadas para actualizarlas, de modo que hasta ese punto las pruebas ayudan a ocuparse de sí mismas.\n",
    "\n",
    "En el peor de los casos, a medida que continúe desarrollando, usted podría encontrar que tiene algunas pruebas que ahora son redundantes. Incluso eso no es un problema, cuando se trata de pruebas, la redundancia es algo bueno.\n",
    "\n",
    "Siempre y cuando las pruebas se organicen de forma razonable, no van a ser difíciles de manejar. Las reglas básicas de uso incluyen el hecho de tener:\n",
    "\n",
    "    una TestClass independiente para cada modelo o vista\n",
    "    un método de prueba independiente para cada conjunto de condiciones que usted quiere probar\n",
    "    nombres de los métodos de prueba que describan su función\n",
    "\n",
    "### Pruebas adicionales\n",
    "\n",
    "Este tutorial sólo presenta algunos de los temas fundamentales de las pruebas. Hay mucho más que usted puede hacer y una serie de herramientas muy útiles a su disposición para lograr algunas cosas muy interesantes.\n",
    "\n",
    "Por ejemplo, aunque nuestras pruebas han tratado aquí algo de la lógica interna de un modelo y la forma en que nuestras vistas publican información, usted puede utilizar un framework «en el navegador» como Selenium para probar la forma en que su HTML en realidad se renderiza en un navegador https://www.selenium.dev/. Estas herramientas le permiten comprobar no sólo el comportamiento de su código Django, sino también, por ejemplo, el de su JavaScript. ¡Es impresionante ver cómo las pruebas ejecutan un navegador y comienzan a interactuar con su sitio como si estuviese siendo operado por un humano! Django incluye LiveServerTestCase para facilitar la integración con herramientas como Selenium.\n",
    "\n",
    "Si usted tiene una aplicación compleja, es posible que desee ejecutar las pruebas de forma automática con cada commit a efectos de la integración continua_, por lo que el control de calidad está en sí, al menos parcialmente, automatizado.\n",
    "\n",
    "Una buena manera de detectar las partes de su aplicación que no han sido probadas es comprobar la cobertura de código. Esto también ayuda a identificar el código frágil o incluso muerto. Si no puede probar un trozo de código, por lo general significa que el código debe ser reestructurado o eliminado. La cobertura ayudará a identificar el código muerto. Consulte Integration with coverage.py para más detalles https://docs.djangoproject.com/es/5.0/topics/testing/advanced/#topics-testing-code-coverage.\n",
    "\n",
    "Las pruebas en Django tiene información completa acerca de las pruebas https://docs.djangoproject.com/es/5.0/topics/testing/.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
